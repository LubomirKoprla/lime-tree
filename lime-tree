import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import manhattan_distances
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

class limeTree:

    def __init__(self,
                 training_data,
                 predict_fn,
                 training_data_labels=None,
                 categorical_features = None,
                 features_names = None):
        """Init function.

                Args:
                    training_data:
                    predict_fn:
                    categorical_features:
                    features_names:
        """
        if categorical_features is None:
            self.categorical_features = []
        else:
            self.categorical_features = list(categorical_features)
        if features_names is None:
            self.features_names = list(range(training_data.shape[0]))
        else:
            self.feature_names = list(features_names)

        # proces numerical atributes
        scaler = StandardScaler(with_mean=False)
        scaler.fit(training_data)
        self.data_mean = scaler.mean_
        self.data_std = scaler.scale_
        self.training_data = training_data
        self.predict_fn = predict_fn
        self.training_data_labels = training_data_labels


    def explain(self,
                data_row,
                tree_classifier=None,
                generate_data=True,
                samples=100,
                generate_data_ratio=0.7):

        if generate_data:
            self.data, self.weights = self.__data_generator(data_row, samples, generate_data_ratio)
        else:
            self.data, self.weights = self.__data_choice(data_row, samples)

        # make prediction on generated data
        self.data_pred = self.predict_fn(self.data)

        if tree_classifier is None:
            tree_classifier = DecisionTreeClassifier(max_depth=2)

        tree_classifier.fit(self.data, self.data_pred, sample_weight=self.weights[0, :])
        return tree_classifier


    def feature_importances(self,
                data_row,
                forest_classifier=None,
                generate_data=True,
                samples=100,
                generate_data_ratio=0.7):

        if generate_data:
            self.data, self.weights = self.__data_generator(data_row, samples, generate_data_ratio)
        else:
            self.data, self.weights = self.__data_choice(data_row,samples)

        # make prediction on generated data
        self.data_pred = self.predict_fn(self.data)

        if forest_classifier is None:
            forest_classifier = RandomForestClassifier(max_depth=2)

        forest_classifier.fit(self.data, self.data_pred, sample_weight=self.weights[0, :])
        return forest_classifier.feature_importances_

    def __data_generator(self,
                         data_row,
                         samples,
                         generate_data_ratio):
        gen_data = []
        distances = []
        row_distance = []
        for i in range(len(self.feature_names)):
            if i in self.categorical_features:
                values = np.random.choice(self.data[:, i], size=samples)
                gen_data.append(values)

                # for weight
                distance = []
                for values in values:
                    if values == data_row[i]:
                        distance.append(0)
                    else:
                        distance.append(1)
                distances.append(distance)
                row_distance.append(0)
            else:
                # x = np.linspace(-20, 20, samples)[np.newaxis].T
                # values_around_row = norm.pdf(x, loc = data_row[i], scale = data_std[i]).sum(axis=1)
                # values_from_data = norm.pdf(x, loc = data_mean[i], scale = data_std[i]).sum(axis=1)

                values_around_row = np.random.normal(loc=data_row[i], scale=self.data_std[i], size=samples*generate_data_ratio)
                values_from_data = np.random.normal(loc=self.data_mean[i], scale=self.data_std[i], size=samples*(1-generate_data_ratio))

                gen_data.append(np.concatenate((values_around_row,values_from_data)))

                # for weight
                distances.append(np.concatenate((values_around_row,values_from_data)))
                row_distance.append(data_row[i])

        gen_data = np.transpose(gen_data)
        distances = np.transpose(distances)
        # weighting generated data
        weights = self.__get_weights(row_distance,distances)
        return gen_data,weights

    def __data_choice(self,
                      data_row,
                      samples):

        new_data = np.random.choice(self.data, size=samples)
        distances = []
        row_distance = []

        for i in range(len(self.feature_names)):
            if i in self.categorical_features:

                distance = []
                for value in new_data[:,i]:
                    if value == data_row[i]:
                        distance.append(0)
                    else:
                        distance.append(1)
                distances.append(distance)
                row_distance.append(0)
            else:

                distances.append(new_data[:,i])
                row_distance.append(data_row[i])
            weights = self.__get_weights(row_distance, distances)

        return new_data, weights


    def __get_weights(self,
                      row_distance,
                      distances):
        return 1 / (manhattan_distances([row_distance], distances) + 1)
